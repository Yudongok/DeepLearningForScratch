{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795de6d5-51b2-40b4-b240-3f313f2a7fc7",
   "metadata": {},
   "source": [
    "# CNN 네트워크 구조\n",
    "\n",
    "![image.png](attachment:7aed6d0b-9652-4c2d-9097-50f71ca4687c:image.png)\n",
    "\n",
    "- 기존 Affine-ReLU연결 방식이 Conv-ReLU-Pooling으로 변경\n",
    "- 출력에 가까운 층에선 Affine-ReLU구성\n",
    "- 마지막 출력계층은 Affine-Softmax\n",
    "\n",
    "**CNN**은 보통 '컨볼루션-풀링' 계층들과 '완전 연결' 계층들의 조합으로 이루어집니다.\n",
    "\n",
    "1. **컨볼루션 계층 (Convolutional Layer)**\n",
    "    - 입력으로 3차원 데이터(채널, 높이, 너비)를 받는다.\n",
    "    - 필터 연산을 통해 **특징 맵(Feature Map)**이라는 3차원 데이터를 출력한다.\n",
    "    - 코드의 `conv_output_size`는 이 특징 맵의 높이와 너비 크기를 계산하는 부분이다.\n",
    "    `(input_size - filter_size + 2*filter_pad) / filter_stride + 1` 공식에 따라 계산되죠.\n",
    "2. **풀링 계층 (Pooling Layer)**\n",
    "    - 컨볼루션 계층에서 나온 특징 맵의 크기를 줄여(sub-sampling) 연산량을 감소시키고 주요 특징을 강조한다.\n",
    "    - 코드에서는 `conv_output_size/2`를 통해 풀링(아마도 2x2 풀링) 후의 크기를 암시하고 있다.\n",
    "3. **완전 연결 계층 (Fully-Connected Layer, Affine Layer)**\n",
    "    - 분류와 같은 최종 작업을 수행하기 위해 사용된다.\n",
    "    - 하지만 이 계층은 1차원 배열 형태의 데이터만 입력으로 받을 수 있다.\n",
    "\n",
    "# CNN용어\n",
    "\n",
    "- 특징 맵: 합성곱 계층의 입출력 데이터\n",
    "- 합성곱 연산: 이미지 처리에서 말하는 필터 연산에 해당한다.\n",
    "- 패딩(Padding): 입력데이터 주변을 특정값(0)으로 채움 → 주로 출력 크기를 조정할 목적으로 사용\n",
    "- 스트라이드(Stride): 필터를 적용하는 위치의 간격 → 스트라이드를 키우면 출력이 작아짐\n",
    "- 풀링: 세로, 가로 방향의 공간을 줄이는 연산. 최대 풀링, 평균 풀링이 있음. 이미지 인식 분야에서는 보통 최대 풀링을 사용.\n",
    "\n",
    "<aside>\n",
    "💡\n",
    "\n",
    "3차원의 합성곱 연산에선 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다.\n",
    "\n",
    "</aside>\n",
    "\n",
    "---\n",
    "\n",
    "# CNN 개념\n",
    "\n",
    "## 블록으로 생각하기\n",
    "\n",
    "채널수 = C, 높이 = H, 너비 = W, 필터 높이 = FH, 필터 너비 = FW\n",
    "\n",
    "입력 데이터의 형상(C, H, W)\n",
    "\n",
    "필터(C, FH, FW)\n",
    "\n",
    "→ 필터가 1개일 경우 출력 데이터도 1개임. 출력 데이터 N개가 필요하면 필터를 N개로 늘리면 됨\n",
    "\n",
    "필터 (FN, C, FH, FW)\n",
    "\n",
    "## 배치 처리\n",
    "\n",
    "N개의 데이터를 배치처리 할 경우 입력 데이터의 차원은 (N, C, H, W)로 바뀜.\n",
    "\n",
    "출력 데이터의 차원은 (N, FN, OH, OW)가 됨.\n",
    "\n",
    "여기서 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이루어진다. 즉, N회분의 처리를 한번에 수행한다.\n",
    "\n",
    "## im2col로 데이터 전개하기\n",
    "\n",
    "im2col이란 입력 데이털르 필터링(가중치 계산)하기 좋게 전개하는 함수이다.\n",
    "\n",
    "1. 입력 데이터에서 필터를 적용하는 영역(3차원 블록)을 한 줄로 늘어놓는다.\n",
    "2. 입력 데이터를 전개한 다음에는 합성곱 계층의 필터(가중치)를 1열로 전개하고, 두 행렬의 곱을 계산하면 된다.\n",
    "3. im2col방식으로 출력한 결과는 2차원 행렬이다. CNN은 데이터를 4차원 배열로 저장하므로 2차원인 출력 데이터를 4차원으로 변형(reshape)한다.\n",
    "\n",
    "### im2col 실제 사용해보기\n",
    "\n",
    "```python\n",
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "```\n",
    "\n",
    "- input_data - (데이터수, 채널수, 높이, 너비)의 4차원 배열\n",
    "- filter_h - 필터의 높이\n",
    "- filter_w - 필터의 너비\n",
    "- stride - 스트라이드\n",
    "- pad - 패딩\n",
    "\n",
    "```python\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W= W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        # 출력 크기 계산\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "\t\t\t\t# 입력 데이터를 im2col로 전개\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        # reshape에서 두 번쨰 인수를 -1로 지정하면 다차원 배열의 원소 수가 변환 후에도 똑같이\n",
    "        # 유지되도록 적절히 묶어줌\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        # transpose함수는 다차원 배열의 축 순서를 바꿔주는 함수이다.\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "```\n",
    "\n",
    "- (10, 3, 5, 5)형상을 한 다차원 배열 w의 원소 수는 총 750개이다. 이 배열에 reshape(10, -1)을 호출하면 750개의 원소를 10묶음으로, 즉 형상이 (10, 75)인 배열로 만들어 준다.\n",
    "\n",
    "## 풀링 계층 구현하기\n",
    "\n",
    "```python\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H - self.pool_h) // self.stride + 1\n",
    "        out_w = (W - self.pool_w) // self.stride + 1\n",
    "\n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # 최댓값 (2)\n",
    "        # np.max메서드는 인수로 축(axis)을 지정할 수 있는데, 이 인수로 지정한 축마다 최댓값을 구할 수 있다.\n",
    "        # 아래와 같은 경우는 입력 xdml 1번째 차원의 축마다 최댓값을 구한다.\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "```\n",
    "\n",
    "1. 입력 데이터를 im2col로 전개\n",
    "2. 행별 최댓값을 구함\n",
    "3. 적절한 모양으로 성형\n",
    "\n",
    "---\n",
    "\n",
    "# CNN 구현하기\n",
    "\n",
    "**[SimpleConvNet]**\n",
    "\n",
    "```python\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, \n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_szie + 2*filter_pad) / filter_stride+1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "```\n",
    "\n",
    "<aside>\n",
    "💡\n",
    "\n",
    "합성곱 계층의 크기를 미리 계산하는 이유?\n",
    "\n",
    "</aside>\n",
    "\n",
    "1. **다음 층(특히 완전연결층, FC layer)과 연결하기 위해**\n",
    "\n",
    "- 완전연결층(FC)은 입력 크기가 **고정된 1차원 벡터**여야 하므로,\n",
    "    \n",
    "    합성곱/풀링을 거친 뒤 **몇 개의 뉴런이 나오나(출력 차원)** 를 알아야 `W`(가중치 행렬)를 올바른 크기로 초기화할 수 있다.\n",
    "    \n",
    "- 즉, `conv_output_size`와 `pool_output_size`를 계산해야 **다음 층 weight shape**를 정할 수 있다.\n",
    "- 컨볼루션과 풀링을 거친 후의 출력 데이터 모양은 `(filter_num, pool_output_height, pool_output_width)`가 됩니다.\n",
    "\n",
    "2. **메모리(파라미터) 초기화에 필요**\n",
    "\n",
    "- 신경망의 각 층 가중치는 `np.random.randn(...) * std` 이런 식으로 초기화한다.\n",
    "- 이때 **행렬 크기**가 정확히 맞지 않으면 에러가 난다.\n",
    "- 그래서 Conv → Pooling 후의 출력 크기를 정확히 알아야,\n",
    "    \n",
    "    `W2`(은닉층), `W3`(출력층) 등 FC 계층의 weight를 미리 올바른 차원으로 잡을 수 있다.\n",
    "    \n",
    "\n",
    "3. **오류 방지 & 가독성**\n",
    "\n",
    "- 합성곱 출력 크기 공식:(I: 입력 크기, F: 필터 크기, P: 패딩, S: 스트라이드)\n",
    "    \n",
    "    ![image.png](attachment:2aa11fcd-d34b-4cf9-81b4-ec63b91e4169:image.png)\n",
    "    \n",
    "- 이걸 코드 안에서 바로 쓰면 실수하기 쉽다.\n",
    "- 따라서 **초기화 단계에서 미리 한 번 계산해 변수로 저장**하면, 이후 네트워크 구현과 디버깅이 훨씬 편해진다.\n",
    "\n",
    "```python\n",
    "# 매개변수 초기화 코드\n",
    "        self.params = {}\n",
    "        # '\\'는 아직 줄이 끝나지 않았다는 의미\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "```\n",
    "\n",
    "학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 나머지 두 완전연결 계츠으이 가중치와 편향이다.\n",
    "\n",
    "이 매개변수들을 인스턴스 변수 params에 저장한다.\n",
    "\n",
    "1번쨰 층의 합성곱 계층의 가중치를 W1, 편향을 b1이라는 키로 저장하고 2번째 층의 완전연결 계층의 가중치와 편향을 W2, b2 마지막 3번쨰 층의 완전연결 계층의 가중치와 편향을 W3와 b3 키로 각각 저장\n",
    "\n",
    "```python\n",
    "# CNN을 구성하는 계층들 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "```\n",
    "\n",
    "**[predict method] → 추론을 수행**\n",
    "\n",
    "```python\n",
    "def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return self.last_layer.forward(y, t)\n",
    "```\n",
    "\n",
    "x는 입력 데이터, t는 정답 레이블이다. 추론을 수행하는 predic메서드는 초기화 때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward메서드를 호출하며 그 결과를 다음 계층에 전달한다. 손실 함수를 구하는 loss메서드는 preict 메서드의 결과를 인수로 마지막 층의 forward메서드를 호출한다. 즉, 첫 계층부터 마지막 계층까지 forward를 처리한다.\n",
    "\n",
    "**[오차역전파법] → 기울기 구하기**\n",
    "\n",
    "```python\n",
    "def gradient(self, x, t):\n",
    "    # 순전파\n",
    "    self.loss(x, t)\n",
    "\n",
    "    # 역전파\n",
    "    dout = 1\n",
    "    dout = self.last_layer.backward(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "        dout = layer.backward(dout)\n",
    "\n",
    "    # 결과 저장\n",
    "    grads = {}\n",
    "    grads['W1'] = self.layers['Conv1'].dW\n",
    "    grads['b1'] = self.layers['Conv1'].db\n",
    "    grads['W2'] = self.layers['Affine1'].dW\n",
    "    grads['b2'] = self.layers['Affine1'].db\n",
    "    grads['W3'] = self.layers['Affine2'].dW\n",
    "    grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "    return grads\n",
    "```\n",
    "\n",
    "`grads` 딕셔너리에 기울기를 저장\n",
    "\n",
    "# CNN 시각화하기\n",
    "\n",
    "## 1번째 층의 가중치 시각화하기\n",
    "\n",
    "위에서 MNIST 데이터셋으로 간단한 CNN학습을 했는데, 그때 1번쨰 층의 합성곱 계층의 가중치는 형상이 (30, 1, 5, 5) 즉, (필터 30개, 채널 1개, 5x5크기)이다.\n",
    "\n",
    "필터가 5x5이고 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화할 수 있다는 뜻이다.\n",
    "\n",
    "![image.png](attachment:63e789ca-c6a8-4197-ac9f-92758579dd64:image.png)\n",
    "\n",
    "학습전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없다. 하지만, 학습을 마친 필터는 규칙성 있는 이미지가 되었다. 흰색에서 검은색으로 변화하는 필터와 덩어리(블롭blob)가 진 필터 등 규칙을 띄는 필터로 바뀌었다.\n",
    "\n",
    "오른쪽같이 규칙성 있는 필터는 에지(색상이 바뀐 경계선)과 블롭(국소적으로 덩어리진 영역)등을 보고있다.\n",
    "\n",
    "## 층 깊이에 따른 추출 정보 변화\n",
    "\n",
    "1번쨰 층의 합성곱 계층에서는 에지나 블롭 등의 저수준 정보가 추출된다. 반면 겹겹이 쌓인 CNN은 계층이 깊어질수록 추출되는 정보(정확히는 강하게 반응하는 뉴런)는 더 추상화 된다.\n",
    "\n",
    "![image.png](attachment:3f169260-cc28-4ffc-bb71-e39e095d5b90:image.png)\n",
    "\n",
    "이 네트워크 구조는 AlexNet이라 한다. 합성곱 계층과 풀링 계층을 여러 겹 쌓고, 마지막으로 완전연결 계층을 거쳐 결과를 출력하는 구조이다. 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다. 처음 층은 단순한 에지에 반응하고, 이어서 텍스처, 그리고 더 복잡한 사물의 일부에 반응하도록 변화한다.\n",
    "\n",
    "즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 ‘고급’정보로 변화해가고 이를 다시 말하면 사물의 ‘의미’를 이해하도록 변화하는 것이다.\n",
    "\n",
    "# 대표적인 CNN\n",
    "\n",
    "## LeNet\n",
    "\n",
    "![image.png](attachment:0e25ee52-62bf-475d-9131-631cead0cec0:image.png)\n",
    "\n",
    "- 손글씨 숫자를 인식하는 네트워크로 1998년에 제안됨\n",
    "- 합성곱 계층과 풀링 계층을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과 출력\n",
    "- LeNet은 시그모이드를 활성화 함수로 사용\n",
    "- 서브샘플링을 하여 중간 데이터의 크기를 줄이지만 현재는 최대 풀링이 주류임\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "![image.png](attachment:88ae2498-b1de-40f3-84e3-da70ae27c84e:image.png)\n",
    "\n",
    "- 2012년에 발표됨\n",
    "- 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전연결 계층을 거침\n",
    "- 활성화 함수를 ReLU로 사용\n",
    "- LRN(Local Response Normalization)이라는 국소적 정규화 실시하는 계층 이용\n",
    "- 드롭아웃 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
